{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Load dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "nspi https://www.kaggle.com/code/ashkhagan/audio-signal-processing-librosa#Visualizing-Audio"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "\n",
    "Il est généralement recommandé de normaliser les données avant l'EDA (Exploratory Data Analysis), car cela peut aider à mieux comprendre la distribution des données et faciliter l'interprétation des résultats de l'analyse exploratoire.\n",
    "\n",
    "La normalisation des données implique de transformer les données pour qu'elles aient une moyenne nulle et une variance unitaire ou dans une plage spécifique. Cette transformation peut aider à éliminer les biais dans les données et à faciliter la comparaison entre différentes caractéristiques.\n",
    "\n",
    "Lorsque les données ne sont pas normalisées, il peut être difficile de déterminer si les différences entre les caractéristiques sont dues à des différences réelles entre les données ou simplement à des différences d'échelle. En normalisant les données, on peut s'assurer que les différences observées sont dues à des différences réelles entre les données et non à des différences d'échelle.\n",
    "\n",
    "Cependant, il est important de noter que la normalisation des données doit être effectuée avec précaution, en fonction du type de données et de l'objectif de l'analyse. Par exemple, pour les données audio, il peut être nécessaire de normaliser les caractéristiques telles que l'amplitude et la fréquence, mais il peut être préférable de ne pas normaliser les caractéristiques liées à la dynamique du son."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EDA\n",
    "\n",
    "Mieux comprendre les données audio, d'identifier les éventuels problèmes et d'optimiser la sélection des caractéristiques audio pour construire un modèle de classification d'audio performant."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset.describe().T"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Class distribution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset['class'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "dataset['class'].value_counts().plot(kind='barh', ax=ax1)\n",
    "dataset['class'].value_counts().plot(kind='pie', ax=ax2, autopct='%.0f%%')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize audio (time, freq domains) by class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_names = dataset['class'].unique().tolist()\n",
    "print(class_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_audio_signals(audio_files, n_fft=2048, hop_length=512, figsize=(20, 8), title=None):\n",
    "    \"\"\"\n",
    "    Affiche les tracés du domaine temporel et du domaine fréquentiel d'une liste d'audios.\n",
    "\n",
    "    Args:\n",
    "        audio_files (list): Liste des chemins des fichiers audio à visualiser (maximum 5).\n",
    "        n_fft (int): Taille de la fenêtre de transformation de Fourier.\n",
    "        hop_length (int): Pas de la fenêtre de transformation de Fourier.\n",
    "        figsize (tuple): Taille de la figure (largeur, hauteur).\n",
    "        title (str): Titre de la figure.\n",
    "    \"\"\"\n",
    "    if len(audio_files) > 5:\n",
    "        raise ValueError(\"Le nombre maximum d'audios pris en charge est de 5.\")\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=len(audio_files), figsize=figsize)\n",
    "    cbar_ax = fig.add_axes([0.925, 0.1, .02, .775]) # [left, bottom, width, height]\n",
    "\n",
    "    for i, audio_file in enumerate(audio_files):\n",
    "        # Chargement du fichier audio\n",
    "        y, sr = librosa.load(audio_file)\n",
    "        audio_name = os.path.basename(audio_file)\n",
    "        # Tracé du domaine temporel\n",
    "        librosa.display.waveshow(y, alpha=0.5, ax=axes[0, i])\n",
    "        axes[0, i].set_title(f\"{audio_name}\", fontsize=10)\n",
    "        axes[0, i].set_ylabel(\"Amplitude\")\n",
    "        axes[0, i].set_xlabel(\"Time (s)\", fontsize=10)\n",
    "\n",
    "        # Tracé du domaine fréquentiel\n",
    "        D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length)\n",
    "        S_db = librosa.amplitude_to_db(abs(D), ref=np.max)\n",
    "        img = librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='log', ax=axes[1, i])\n",
    "        axes[1, i].set_title(f\"{audio_name}\", fontsize=10)\n",
    "        axes[1, i].set_xlabel(\"Time (s)\", fontsize=10)\n",
    "        axes[1, i].set_ylabel(\"Frequency (Hz)\")\n",
    "        #axes[1, i].set_yticks([100, 200, 500, 1000, 2000, 5000])\n",
    "        #axes[1, i].set_yticklabels(['100', '200', '500', '1k', '2k', '5k'])\n",
    "\n",
    "    # Ajout de la barre de couleur\n",
    "    plt.colorbar(img, format=\"%+2.0f dB\", ax=axes.ravel().tolist(), cax=cbar_ax)\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.3)\n",
    "    if title:\n",
    "        plt.suptitle(title, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "for class_name in class_names:\n",
    "    # get all file_path for this class\n",
    "    file_paths = dataset[dataset['class'] == class_name]['file_path'].tolist()\n",
    "    random.shuffle(file_paths)\n",
    "    plot_audio_signals(file_paths[:5], title=class_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset.boxplot()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Correlation\n",
    "\n",
    "Cette étape consiste à explorer les relations entre les caractéristiques audio en calculant les corrélations entre elles. Cela permet de mieux comprendre les relations entre les différentes caractéristiques et de détecter les"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
