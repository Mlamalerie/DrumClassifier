{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Cleaning\n",
    "Cette étape est cruciale pour garantir la qualité et la fiabilité des données avant de les utiliser pour l'analyse et la modélisation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:00:16.861090Z",
     "end_time": "2023-04-24T00:00:18.316006Z"
    }
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "from scipy.signal import hilbert\n",
    "import numpy as np\n",
    "from scipy.stats import skew\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "# change style\n",
    "plt.style.use('ggplot')\n",
    "import librosa\n",
    "import librosa.display\n",
    "from tqdm import tqdm\n",
    "from tools import play_audio, load_audio_file, pad_signal\n",
    "\n",
    "tqdm.pandas()\n",
    "import os\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from params import SOUNDS_DATASET_PATH, SAMPLE_RATE, CLASS_COLORS\n",
    "from tools import play_audio, load_audio_file, pad_signal"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:00:16.943775Z",
     "end_time": "2023-04-24T00:00:18.317009Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_csv_path = os.path.join(SOUNDS_DATASET_PATH, 'dataset_handcrafted_features_extracted.csv')\n",
    "df_drums = pd.read_csv(dataset_csv_path)\n",
    "\n",
    "df_drums = df_drums.head(200)\n",
    "# drop 'Unnamed: 0' column\n",
    "df_drums.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "# set index to file_path\n",
    "df_drums.set_index('file_path', inplace=True)\n",
    "print(\"Dataset shape: \", df_drums.shape)\n",
    "df_drums"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:00:16.954125Z",
     "end_time": "2023-04-24T00:00:18.578194Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_drums.info(verbose=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:00:17.813631Z",
     "end_time": "2023-04-24T00:00:18.578742Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_drums.describe().T"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:00:18.006596Z",
     "end_time": "2023-04-24T00:00:18.798518Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Keep only float columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get columns names with float type use .info()\n",
    "columns_float = [k for k, v in df_drums.dtypes.to_dict().items() if v == 'float64' or v == 'int64']\n",
    "columns_float"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:00:18.569439Z",
     "end_time": "2023-04-24T00:00:18.930170Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Delete Full of NaN rows"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get rows with all NaN values\n",
    "df_drums[df_drums.isna().all(axis=1)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:00:18.592229Z",
     "end_time": "2023-04-24T00:00:18.989296Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# delete rows with all NaN values (focus on features)\n",
    "df_drums.dropna(how='all', subset=columns_float, inplace=True)\n",
    "df_drums"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:00:18.636884Z",
     "end_time": "2023-04-24T00:00:18.991297Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Duplicates\n",
    "Suppression des duplicats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Duplicates rows"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Nombre de lignes totalement dupliquées : {df_drums.duplicated().sum()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:00:18.717140Z",
     "end_time": "2023-04-24T00:00:19.237384Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "duplicated_focus_on_features = df_drums.duplicated(subset=columns_float)\n",
    "print(\n",
    "    f\"Nombre de lignes dupliquées (focus on features) : {duplicated_focus_on_features.sum()} lignes (qu'on peut potentiellement supprimer)\")\n",
    "\n",
    "# print per class\n",
    "df_drums[duplicated_focus_on_features].groupby(\"class\").count()[\"file_name\"].sort_values(ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:00:18.788342Z",
     "end_time": "2023-04-24T00:00:19.240388Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "duplicated_focus_on_features = df_drums.duplicated(subset=columns_float, keep=False)\n",
    "# Afficher les lignes dupliquées (toutes les copies)\n",
    "duplicates_df = df_drums[duplicated_focus_on_features].sort_values(by=columns_float)\n",
    "duplicates_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:00:18.874331Z",
     "end_time": "2023-04-24T00:00:20.178817Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Regroupez les lignes en double en fonction de leurs valeurs de features\n",
    "grouped_duplicates = duplicates_df.groupby(columns_float)\n",
    "\n",
    "# Créez une liste contenant des listes de file_paths pour chaque groupe de duplicatas\n",
    "duplicate_groups = []\n",
    "for _, group in grouped_duplicates:\n",
    "    duplicate_groups.append(list(group.index))\n",
    "\n",
    "# Affichez les groupes de duplicatas\n",
    "for i, group in enumerate(duplicate_groups):\n",
    "    print(f\"# Duplicate Group {i + 1}:\")\n",
    "    for file_path in group:\n",
    "        print(f\"  - {file_path}\")\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:00:19.025840Z",
     "end_time": "2023-04-24T00:00:20.612391Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "maintenant qu'on a les lignes dupliquées, on va les supprimer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Delete duplicates in dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "duplicates_file_to_delete = []\n",
    "file_to_delete_num_group_map = {}\n",
    "\n",
    "# Parcourez chaque groupe de doublons\n",
    "for num_group, group in enumerate(duplicate_groups, start=1):\n",
    "    # Triez les fichiers audio du groupe par la taille de leur nom de fichier\n",
    "    file_name_len = lambda file_path: len(os.path.basename(file_path))\n",
    "    sorted_group = sorted(group, key=file_name_len)\n",
    "\n",
    "    # Gardez le fichier audio avec le plus petit nom de fichier (le premier de la liste triée)\n",
    "    to_keep = sorted_group[0]\n",
    "\n",
    "    # Ajoutez les autres fichiers audio du groupe à la liste des fichiers à supprimer\n",
    "    duplicates_file_to_delete.extend(sorted_group[1:])\n",
    "\n",
    "    for file_path in sorted_group[1:]:\n",
    "        file_to_delete_num_group_map[file_path] = num_group\n",
    "\n",
    "print(len(set(duplicates_file_to_delete)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:00:19.202933Z",
     "end_time": "2023-04-24T00:00:20.613390Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Copiez votre DataFrame pour ne pas modifier l'original\n",
    "cleaned_df_drums = df_drums.copy()\n",
    "# Supprimez les autres fichiers audio du groupe de doublons du DataFrame\n",
    "cleaned_df_drums = cleaned_df_drums.drop(file_path for file_path in duplicates_file_to_delete)\n",
    "\n",
    "# cleaned_df contient maintenant les données sans les doublons indésirables\n",
    "print(\n",
    "    f\"Nombre de lignes dupliquées (focus on features) : {cleaned_df_drums.duplicated(subset=columns_float).sum()} lignes (qu'on peut potentiellement supprimer)\")\n",
    "cleaned_df_drums"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:00:19.221623Z",
     "end_time": "2023-04-24T00:00:20.861772Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "sauvegarder dans un fichier csv les données dupliquées (pour les supprimer manuellement)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save duplicates in txt file\n",
    "with open(os.path.join(SOUNDS_DATASET_PATH, \"__duplicates.txt\"), \"w\") as f:\n",
    "    for file_path in duplicates_file_to_delete:\n",
    "        f.write(f\"{file_path} {file_to_delete_num_group_map[file_path]}\" + os.linesep)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:00:19.322112Z",
     "end_time": "2023-04-24T00:00:20.862780Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Duplicates file_name"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Nombre de lignes dupliquées sur la colonne 'file_name' : {cleaned_df_drums.duplicated(subset=['file_name', 'file_extension']).sum()} lignes (qu'on peut potentiellement supprimer)\")\n",
    "\n",
    "cleaned_df_drums[cleaned_df_drums.duplicated(subset=['file_name', 'file_extension'], keep=False)].sort_values(\n",
    "    by=['file_name', 'file_extension'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:00:19.386446Z",
     "end_time": "2023-04-24T00:00:20.864924Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Duplicates (too similar) audio content\n",
    "\n",
    "Pour détecter les fichiers audio dupliqués,  on va comparer les caractéristiques audio de chaque fichier audio. Si les caractéristiques audio sont identiques, alors les fichiers audio sont dupliqués."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# euclidean distance with np.linalg.norm\n",
    "def euclidean_distance(vector1: np.ndarray, vector2: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute euclidean distance between two vectors\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(vector1 - vector2)\n",
    "\n",
    "\n",
    "# cosine similarity with np.dot\n",
    "def cosine_similarity(vector1: np.ndarray, vector2: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two vectors\n",
    "    \"\"\"\n",
    "    return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "\n",
    "\n",
    "def similarity(vector1: np.ndarray, vector2: np.ndarray, metric: str = \"euclidean\"):\n",
    "    \"\"\"\n",
    "    Compute similarity between two vectors using the specified metric. Use numpy functions.\n",
    "    \"\"\"\n",
    "    if metric == \"cosine\":\n",
    "        sim = cosine_similarity(vector1, vector2)\n",
    "        return sim\n",
    "        # Normalize cosine similarity to [0, 1]\n",
    "        #return (sim + 1) / 2\n",
    "    elif metric == \"euclidean\":\n",
    "        dist = euclidean_distance(vector1, vector2)\n",
    "        # Normalize euclidean distance to [0, 1] by dividing by the maximum possible distance\n",
    "        max_dist = np.sqrt(len(vector1))\n",
    "        return 1 - (dist / max_dist)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown metric: {metric}\")\n",
    "\n",
    "\n",
    "def compute_similarities(df_X: pd.DataFrame, metric: str = \"euclidean\"):\n",
    "    \"\"\"\n",
    "    Compute similarities between audio files\n",
    "    \"\"\"\n",
    "    similarities = {}\n",
    "\n",
    "    for file_i, row_i in tqdm(df_X.iterrows(), total=len(df_X)):\n",
    "        for file_j, row_j in df_X.iterrows():\n",
    "            if file_i == file_j:\n",
    "                continue\n",
    "            if (file_i, file_j) in similarities or (file_j, file_i) in similarities:\n",
    "                continue\n",
    "            vectori = row_i.to_numpy()\n",
    "            vectorj = row_j.to_numpy()\n",
    "            similarities[(file_i, file_j)] = similarity(vectori, vectorj, metric=metric)\n",
    "\n",
    "    return similarities\n",
    "\n",
    "\n",
    "def compute_df_similarities(df_X: pd.DataFrame, metric: str = \"euclidean\"):\n",
    "    \"\"\"\n",
    "    Compute similarities between audio files\n",
    "    \"\"\"\n",
    "    similarities = compute_similarities(df_X, metric=metric)\n",
    "\n",
    "    # crate a dataframe with similarities dict\n",
    "    df_similarities = pd.DataFrame.from_dict(similarities, orient='index', columns=['similarity']).sort_values(\n",
    "        by=['similarity'], ascending=False)\n",
    "    # from index (tuple), create 2 column file_i and file_j\n",
    "    df_similarities.reset_index(inplace=True)\n",
    "    df_similarities[\"file_i\"] = df_similarities[\"index\"].apply(lambda x: x[0])\n",
    "    df_similarities[\"file_j\"] = df_similarities[\"index\"].apply(lambda x: x[1])\n",
    "    df_similarities.drop(columns=[\"index\"], inplace=True)\n",
    "\n",
    "    return df_similarities\n",
    "\n",
    "\n",
    "df_similarities = compute_df_similarities(cleaned_df_drums[columns_float], metric=\"cosine\")\n",
    "df_similarities"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:13:22.849062Z",
     "end_time": "2023-04-24T00:13:27.205493Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Similary > 0.999"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_similarities_0_999 = df_similarities.query(\"similarity > 0.999\")\n",
    "df_similarities_0_999"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:13:28.588434Z",
     "end_time": "2023-04-24T00:13:28.697345Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# play some audio files with similarity > 0.999\n",
    "for i, row in df_similarities_0_999.sample(10).iterrows():\n",
    "    print(f\"#### similarity: {row['similarity']}\")\n",
    "    play_audio(row[\"file_i\"])\n",
    "    play_audio(row[\"file_j\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:00:24.696425Z",
     "end_time": "2023-04-24T00:00:25.762836Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Delete similar files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pandas display full table\n",
    "#pd.set_option('display.max_rows', None)\n",
    "table_loser_0_999 = pd.Series(\n",
    "    df_similarities_0_999[\"file_i\"].to_list() + df_similarities_0_999[\"file_j\"].to_list()).value_counts()\n",
    "table_loser_0_999"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:13:54.540529Z",
     "end_time": "2023-04-24T00:13:54.644378Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "play_audio(\"G:\\Shared drives\\PFE - ING3 Mlamali\\DrumClassifier - Sounds Dataset\\Conga\\Conga (119).wav\")\n",
    "play_audio(\"G:\\Shared drives\\PFE - ING3 Mlamali\\DrumClassifier - Sounds Dataset\\Conga\\Conga (118).wav\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:09:20.489308Z",
     "end_time": "2023-04-24T00:09:20.653258Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similar_file_to_delete = []  # list of file to delete\n",
    "for i, row in df_similarities_0_999.iterrows():\n",
    "    if row[\"file_i\"] in similar_file_to_delete or row[\"file_j\"] in similar_file_to_delete:\n",
    "        continue\n",
    "    if table_loser_0_999[row[\"file_j\"]] > table_loser_0_999[row[\"file_i\"]]:\n",
    "        #print(f\"{os.path.basename(row['file_j'])} lose vs. {os.path.basename(row['file_i'])} (because {table_loser_0_999[row['file_j']]} > {table_loser_0_999[row['file_i']]})\")\n",
    "        similar_file_to_delete.append(row[\"file_j\"])\n",
    "    else:\n",
    "        #print(f\"{os.path.basename(row['file_i'])} lose vs. {os.path.basename(row['file_j'])} (because {table_loser_0_999[row['file_i']]} > {table_loser_0_999[row['file_j']]})\")\n",
    "        similar_file_to_delete.append(row[\"file_i\"])\n",
    "\n",
    "len(similar_file_to_delete)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:10:39.275762Z",
     "end_time": "2023-04-24T00:10:39.450249Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# delete similar files from cleaned_df_drums\n",
    "cleaned_df_drums = cleaned_df_drums.drop(similar_file_to_delete)\n",
    "cleaned_df_drums"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:01:11.781119Z",
     "end_time": "2023-04-24T00:01:11.816410Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "on affiche maintenant les fichiers qui ont été supprimés, pour aller les supprimer manuellement"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similar_file_to_delete"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Missing values Processing\n",
    "Traitement des valeurs manquantes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cleaned_df_drums.isnull().any()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:18:12.105356Z",
     "end_time": "2023-04-24T00:18:13.147009Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cleaned_df_drums.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:19:52.785090Z",
     "end_time": "2023-04-24T00:19:53.200840Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cleaned_df_drums.describe().T"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:20:58.876138Z",
     "end_time": "2023-04-24T00:21:01.911135Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cleaned_df_drums.loc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:20:20.010000Z",
     "end_time": "2023-04-24T00:20:20.460899Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Outliers\n",
    "\n",
    "Un outlier (ou valeur aberrante) est une observation qui se situe à une distance anormalement grande des autres observations dans un ensemble de données. Les outliers peuvent être causés par des erreurs de mesure, des erreurs d'enregistrement, ou par des variations naturelles dans les données. Ils peuvent avoir un impact significatif sur l'analyse et la modélisation des données, en introduisant des biais et en réduisant la performance des modèles prédictifs.\n",
    "\n",
    "Dans le contexte de notre projet, les outliers peuvent correspondre à des sons de batterie ayant des caractéristiques très différentes des autres sons, qui pourraient rendre difficile la classification ou l'analyse ultérieure."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cleaned_df_drums.describe().T"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:26:40.682666Z",
     "end_time": "2023-04-24T00:26:42.724899Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cleaned_df_drums[\"duration\"].boxplot()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:36:02.515543Z",
     "end_time": "2023-04-24T00:36:33.767857Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def get_outliers_iqr_per_class(df, column, class_column, multiplier=1.5):\n",
    "    outliers_indices = []\n",
    "\n",
    "    # Divisez le dataframe en sous-groupes en fonction des classes.\n",
    "    for class_value in df[class_column].unique():\n",
    "        class_df = df[df[class_column] == class_value]\n",
    "\n",
    "        # Appliquez la méthode IQR pour chaque sous-groupe.\n",
    "        Q1 = class_df[column].quantile(0.25)\n",
    "        Q3 = class_df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - multiplier * IQR\n",
    "        upper_bound = Q3 + multiplier * IQR\n",
    "        outliers_class_df = class_df[(class_df[column] < lower_bound) | (class_df[column] > upper_bound)]\n",
    "\n",
    "        outliers_indices.extend(outliers_class_df.index.tolist())\n",
    "\n",
    "    return outliers_indices\n",
    "\n",
    "\n",
    "# Remplacez 'class_column' par le nom de la colonne contenant les classes dans votre dataframe.\n",
    "class_column = 'class'\n",
    "\n",
    "# Parcourez toutes les colonnes de cleaned_df_drums pour lesquelles vous souhaitez détecter les outliers.\n",
    "outliers_counter = Counter()\n",
    "for col in tqdm(columns_float):\n",
    "    outliers_indices = get_outliers_iqr_per_class(cleaned_df_drums, col, class_column)\n",
    "    outliers_counter.update(outliers_indices)\n",
    "\n",
    "limit_outliers_count = 15\n",
    "# Trouvez les index des lignes qui ont été détectées au moins 3 fois comme outliers.\n",
    "outliers_to_remove = [index for index, count in outliers_counter.items() if count >= limit_outliers_count]\n",
    "print(f\"Nombre d'outliers à supprimer: {len(outliers_to_remove)}\")\n",
    "\n",
    "for file_outlier in outliers_to_remove[:10]:\n",
    "    play_audio(file_outlier)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T01:07:01.565441Z",
     "end_time": "2023-04-24T01:07:07.744901Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Supprimez les outliers du dataframe.\n",
    "cleaned_df_drums = cleaned_df_drums.drop(outliers_to_remove)\n",
    "print(f\"Nombre de lignes restantes: {len(cleaned_df_drums)}\")\n",
    "cleaned_df_drums"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save cleaned dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cleaned_df_drums.to_csv(os.path.join(SOUNDS_DATASET_PATH, \"dataset_cleaned_and_features.csv\"), index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T01:10:38.960271Z",
     "end_time": "2023-04-24T01:10:40.589485Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
